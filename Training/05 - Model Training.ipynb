{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training\n",
    "This script trains the model to predict the marker positions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import math\n",
    "\n",
    "from sklearn.utils import shuffle \n",
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPool2D\n",
    "from keras import backend as K\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.initializers import glorot_uniform\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "from keras import metrics\n",
    "\n",
    "import h5py\n",
    "\n",
    "import datetime\n",
    "import time\n",
    "\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.callbacks import CSVLogger\n",
    "\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing training and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "HDF5_PATH = \"data.hdf5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdf = h5py.File(HDF5_PATH, \"r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2083884, 21, 3) (2083884, 63) (534252, 21, 3) (534252, 63)\n"
     ]
    }
   ],
   "source": [
    "train_x = hdf[\"train/data\"]\n",
    "train_y = hdf[\"train/labels\"]\n",
    "\n",
    "test_x = hdf[\"test/data\"]\n",
    "test_y = hdf[\"test/labels\"]\n",
    "\n",
    "print(train_x.shape, train_y.shape, test_x.shape, test_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "startTime = str(round(time.time() * 1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log(s):\n",
    "    with open(\"status_Training_imageSingleHandNoR.\" + startTime + \".txt\", \"a\") as myfile:\n",
    "        myfile.write(\"[\" + str(datetime.datetime.now()) + \"],\" + s + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def addRandomNoise(gs, ls, batch_size):\n",
    "    for i in range(0, batch_size):\n",
    "        random_array = np.random.randint(-2,3,(63))\n",
    "        gs[i] = gs[i] + random_array\n",
    "        ls[i] = ls[i] + (random_array / 1000)\n",
    "        \n",
    "    return {'gs':gs, 'ls':ls}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeMarkersRandomly(coordinates):\n",
    "    for hand in range(0, len(coordinates)):\n",
    "        numOfMarkersToRemove = random.randint(0, 4)\n",
    "        for i in range(0, numOfMarkersToRemove):\n",
    "            indexToRemove = random.randint(0, (len(coordinates[hand]) - 1))\n",
    "            coordinates[hand].pop(indexToRemove)\n",
    "            \n",
    "    return coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def addGhostMarkers(coordinates):\n",
    "    for hand in range(0, len(coordinates)):\n",
    "        numOfMarkersToAdd = random.randint(0, 4)\n",
    "        for i in range(0, numOfMarkersToAdd):\n",
    "            x = random.randint(0, nop - 1)\n",
    "            z = random.randint(0, nop - 1)\n",
    "            y = random.randint(-ultimateY, ultimateY)\n",
    "            coordinates[hand].append([x,y,z])\n",
    "            \n",
    "    return coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def myGeneratorImage(set_name, batch_size):\n",
    "    hdf = h5py.File(HDF5_PATH, \"r\")\n",
    "    # in mm\n",
    "    pData = hdf[set_name + \"/data\"]\n",
    "    # in m\n",
    "    pLabels = hdf[set_name + \"/labels\"]\n",
    "    \n",
    "    len_train = pData.shape[0]\n",
    "    randomBatchOrder = np.arange(len_train // batch_size)    \n",
    "    \n",
    "       \n",
    "    while True:\n",
    "        np.random.shuffle(randomBatchOrder) \n",
    "        \n",
    "        for i in range(len_train // batch_size):\n",
    "            idx = randomBatchOrder[i]\n",
    "            \n",
    "            # read data and labels for current batch\n",
    "            gs = pData[idx * batch_size: (idx+1) * batch_size]\n",
    "            ls = pLabels[idx * batch_size: (idx+1) * batch_size]\n",
    "            \n",
    "            gs = np.array(gs)\n",
    "            gs = gs.reshape(batch_size, 63)\n",
    "            ls = np.array(ls)\n",
    "            \n",
    "            noisy = addRandomNoise(gs, ls, batch_size)\n",
    "            gs = noisy['gs']\n",
    "            gs = gs.reshape(-1, 21, 3)\n",
    "            ls = noisy['ls']\n",
    "            ls = ls * 1000\n",
    "\n",
    "            \n",
    "            # remove y from ls\n",
    "            ls = ls.reshape(-1)\n",
    "            ls = list(ls)\n",
    "            del ls[1::3]\n",
    "            ls = np.array(ls)\n",
    "            ls = ls.reshape(-1,21 * 2)\n",
    "            \n",
    "            \n",
    "            gs = createImageSingle(gs)\n",
    "\n",
    "            shuffled = shuffle(gs, ls)\n",
    "            yield shuffled[0], shuffled[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "resolutionPcm = 4\n",
    "resolutionPmm = resolutionPcm / 10\n",
    "\n",
    "# image size in cm\n",
    "imageSize = 25\n",
    "# max y in mm\n",
    "ultimateY = 120\n",
    "\n",
    "# number of pixels\n",
    "nop = imageSize * resolutionPcm\n",
    "zzz = [nop / 2, 0, nop - (8 * resolutionPcm)]\n",
    "\n",
    "def createImageSingle(coordinates): # coordinates in mm\n",
    "    images = np.zeros((coordinates.shape[0], nop, nop, 1))\n",
    "    \n",
    "    coordinates = (coordinates * resolutionPmm) + zzz\n",
    "    \n",
    "    # take care that values fit to image size\n",
    "    coordinates = coordinates.reshape(-1)\n",
    "    xt = coordinates[0::3]\n",
    "    yt = coordinates[1::3]\n",
    "    zt = coordinates[2::3]\n",
    "    xt[np.where(xt < 0)] = 0\n",
    "    zt[np.where(zt < 0)] = 0\n",
    "    xt[np.where(xt >= nop)] = nop - 1\n",
    "    zt[np.where(zt >= nop)] = nop - 1\n",
    "\n",
    "    # remove resolutionPmm from y again\n",
    "    yt = (yt / resolutionPmm)\n",
    "    # set max value for y to ultimateY\n",
    "    yt[np.where(yt > ultimateY)] = ultimateY\n",
    "    yt[np.where(yt < -ultimateY)] = -ultimateY\n",
    "    # normalize yt\n",
    "    yt = (yt / ultimateY)\n",
    "    coordinates[0::3] = xt\n",
    "    coordinates[1::3] = yt\n",
    "    coordinates[2::3] = zt\n",
    "    coordinates = coordinates.reshape(-1, 21, 3)\n",
    "    \n",
    "    coordinates = coordinates.tolist()\n",
    "    # remove markers randomly\n",
    "    coordinates = removeMarkersRandomly(coordinates)\n",
    "    # add random ghost markers\n",
    "    coordinates = addGhostMarkers(coordinates)\n",
    "    \n",
    "    \n",
    "    # coordinates as float\n",
    "    coordinatesF = coordinates\n",
    "    # coordinates as int\n",
    "    coordinates = [[[int(z) for z in y] for y in x] for x in coordinates]\n",
    "    for i in range(0, len(coordinates)):\n",
    "        coords = coordinates[i]\n",
    "        coordsF = coordinatesF[i]\n",
    "        \n",
    "        for j in range(0, len(coords)):\n",
    "            # xz-plane image with y value\n",
    "            images[i][coords[j][0]][coords[j][2]][0] = coordsF[j][1]# y values are normalized\n",
    "            \n",
    "    return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class My_Callback(keras.callbacks.Callback):\n",
    "    def on_train_end(self, logs={}):\n",
    "        log(\"Training ended\")\n",
    "        return\n",
    " \n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        log(\"Ending Epoch\")\n",
    "        log(str(logs))\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 150\n",
    "num_output = 21 * 2 # (x,z) for 21 markers! No R_Shapes!\n",
    "epochs = 10000 \n",
    "\n",
    "\n",
    "# the data, shuffled and split between train and test sets\n",
    "(x_train, y_train), (x_test, y_test) = (train_x, train_y), (test_x, test_y)\n",
    "\n",
    "\n",
    "###########################################\n",
    "## GPU training configuration\n",
    "###########################################\n",
    "config = tf.ConfigProto(log_device_placement=True, allow_soft_placement=True, device_count = {'GPU' : 4})\n",
    "config.gpu_options.allow_growth=True\n",
    "config.gpu_options.per_process_gpu_memory_fraction=1\n",
    "config.gpu_options.allocator_type = 'BFC'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10000\n",
      "16029/16029 [==============================] - 6787s 423ms/step - loss: 64.8719 - rmse: 64.8719 - val_loss: 58.9132 - val_rmse: 58.9132\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 58.91318, saving model to weights.best.imageSingleHandNoR.1542827622775.hdf5\n",
      "Epoch 2/10000\n",
      "16029/16029 [==============================] - 6774s 423ms/step - loss: 55.5422 - rmse: 55.5422 - val_loss: 44.5851 - val_rmse: 44.5851\n",
      "\n",
      "Epoch 00002: val_loss improved from 58.91318 to 44.58513, saving model to weights.best.imageSingleHandNoR.1542827622775.hdf5\n",
      "Epoch 3/10000\n",
      "  173/16029 [..............................] - ETA: 1:40:57 - loss: 48.1495 - rmse: 48.1495"
     ]
    }
   ],
   "source": [
    "with tf.device('/gpu:0'):\n",
    "    session = tf.Session(config=config)\n",
    "    K.set_session(session)\n",
    "    input_shape = (nop, nop, 1)\n",
    "\n",
    "    ###########################################\n",
    "    ## Model architecture \n",
    "    ###########################################\n",
    "    model = Sequential()\n",
    "\n",
    "    # First layer of convolution and max pooling\n",
    "    model.add(Conv2D(filters = 64, kernel_size = (6,6),padding = 'Same', \n",
    "                     activation ='relu', input_shape = input_shape))\n",
    "    model.add(BatchNormalization(axis=-1))\n",
    "    model.add(Conv2D(filters = 64, kernel_size = (3,3),padding = 'Same', \n",
    "                     activation ='relu'))\n",
    "    model.add(BatchNormalization(axis=-1))\n",
    "    model.add(Conv2D(filters = 64, kernel_size = (3,3),padding = 'Same', \n",
    "                     activation ='relu'))\n",
    "    model.add(BatchNormalization(axis=-1))\n",
    "    model.add(MaxPool2D(pool_size=(4,4)))\n",
    "    model.add(Dropout(0.5))\n",
    "    \n",
    "    # Second layer of convolution and max pooling\n",
    "    model.add(Conv2D(filters = 128, kernel_size = (3,3),padding = 'Same', \n",
    "                     activation ='relu'))\n",
    "    model.add(BatchNormalization(axis=-1))\n",
    "    model.add(Conv2D(filters = 128, kernel_size = (3,3),padding = 'Same', \n",
    "                     activation ='relu'))\n",
    "    model.add(BatchNormalization(axis=-1))\n",
    "    model.add(MaxPool2D(pool_size=(2,2), strides=(2,2)))\n",
    "    model.add(Dropout(0.5))\n",
    "\n",
    "    # Dense layer\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(512, activation = \"relu\", kernel_initializer=glorot_uniform()))\n",
    "    model.add(BatchNormalization(axis=-1))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(512, activation = \"relu\", kernel_initializer=glorot_uniform()))\n",
    "    model.add(BatchNormalization(axis=-1))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(256, activation = \"relu\", kernel_initializer=glorot_uniform()))\n",
    "    model.add(BatchNormalization(axis=-1))\n",
    "    model.add(Dropout(0.5))\n",
    "    \n",
    "    # Output layer \n",
    "    model.add(Dense(num_output)) \n",
    "    \n",
    "    ###########################################\n",
    "    ## Model training \n",
    "    ###########################################     \n",
    "    def rmse(y_true, y_pred):\n",
    "        return K.sqrt(K.mean(K.square(y_pred - y_true), axis=-1)) \n",
    "        \n",
    "    optimizer = keras.optimizers.Adam(lr=0.00001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "    model.compile(loss=rmse,\n",
    "                  optimizer=optimizer,\n",
    "                  metrics=[rmse])\n",
    "\n",
    "    learning_rate_reduction = ReduceLROnPlateau(monitor='val_loss', \n",
    "                                            patience=3, \n",
    "                                            verbose=1, \n",
    "                                            factor=0.5, \n",
    "                                            min_lr=0.0000001)\n",
    "    \n",
    "    filepath=\"weights.best.imageSingleHandNoR.\" + startTime + \".hdf5\"\n",
    "    checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='auto')\n",
    "    \n",
    "    customCB = My_Callback()\n",
    "    model.fit_generator(myGeneratorImage(\"train\", batch_size),\n",
    "                        steps_per_epoch=len(train_x) // batch_size,\n",
    "                        epochs=epochs,\n",
    "                        verbose=1,\n",
    "                        validation_data=myGeneratorImage(\"test\", batch_size),\n",
    "                        validation_steps=len(test_x) // batch_size,\n",
    "                        callbacks=[checkpoint, customCB, learning_rate_reduction])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
